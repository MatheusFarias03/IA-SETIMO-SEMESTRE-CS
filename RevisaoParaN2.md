# Intelig√™ncia Artificial - Revis√£o Para a N2

## Aprendizado Supervisionado
Trata de dois tipos de problemas ou tarefas: classifica√ß√£o ou regress√£o. √â uma abordagem de aprendizado de m√°quina definida pelo uso de conjunto de dados rotulados. √â o mapeamento do conjunto de dados de entrada (vari√°veis preditoras) e as respectivas categorias (r√≥tulos) que podem ser ent√£o os novos casos para a predi√ß√£o.

* Regress√£o: predi√ß√£o de valores.
* Classifica√ß√£o: classifica√ß√£o em categorias.

## Aprendizado n√£o supervisionado
Usa algor√≠tmos de aprendizado de m√°quina para analisar e agrupar conjuntos de dados n√£o rotulados. A id√©ia desses algor√≠tmos √© encontrar padr√µes ocultos nos dados **sem a necessidade de qualquer interven√ß√£o humana ou r√≥tulos pr√©-definidos e, por isso, n√£o supervisionados.**

## Modelos de Classifica√ß√£o
Modelos de Classifica√ß√£o estimam a partir dos atributos de entrada categorias. 

### Classificador Log√≠stico (Regress√£o Log√≠stica)
A Regress√£o Log√≠stica modela as probabilidades para problemas de classifica√ß√£o bin√°rios, com dois resultados poss√≠veis, como yes/no, true/false, fraude/n√£o fraude, spam/n√£o spam ou 0/1, e pode ser entendido como uma extens√£o dos modelos de regress√£o linear para problemas de classifica√ß√£o

**Importante: A Regress√£o Log√≠stica √© um classificador Bin√°rio, isto √©, ele s√≥ classifica categorias Dicot√¥micas, como yes/no, true/false!**

Para obter essa probabilidade a regress√£o log√≠stica buscar√° os melhores coeficientes em uma express√£o semelhante a empregada em regress√£o linear. √â uma express√£o que emprega uma combina√ß√£o linear das vari√°veis preditoras √† qual aplicamos a fun√ß√£o log√≠stica ùúé(ùë•)=1/(1+ùëí^(‚àíùë•)) 

```
ùëù = 1/(1+ùëí^(‚àí(ùëé0+ùëé1ùë•1+...+ùëéùëõùë•ùëõ))) 
```

### M√©tricas

#### Matriz de Confus√£o
A matriz de confus√£o √© uma matriz que sumariza os resultados de acertos e erros do modelo para avaliar o desempenho da classifica√ß√£o. √â uma matriz quadrada,  ùëõ√óùëõ  onde  ùëõ  √© o n√∫mero de classes objetivo. Lembrando que avaliamos o modelo sobre os resultados no conjunto de teste, a matriz compara os valores reais (conjunto de teste) com aqueles estimados pelo modelo

#### Acuracidade
$$ Accuracy = \frac{Total de Acertos}{Total de Casos} $$

#### Precis√£o
A Precis√£o √© um valor que, dados todos elementos previstos uma classe, quantos foram previstos corretamente. Isto √©, o percentual dos casos que de fato pertencem √†quela classe. 

$$ Precision = \frac{TP}{TP + FP} $$

#### Recall
O Recall (Revoca√ß√£o, ou Sensibilidade) por outro lado nos diz quantos casos de uma determinada classe foram corretamente previstos.

$$ Recall = \frac{TP}{TP + FN} $$

#### F1-Score
O F1-score pode ser entendido como uma m√©dia harm√¥nica dos valores de precis√£o e recall:

$$ F1-score = \frac{2}{1/Recall + 1/Precision}$$

Na pr√°tica, quando tentamos aumentar a precis√£o do nosso modelo, o recall diminui e vice-versa. A pontua√ß√£o F1 permite capturar ambas as tend√™ncias em um √∫nico valor e, por isso √© bastante empregada sendo seu valor m√°ximo quando a precis√£o e o recall s√£o iguais.

#### Classification Report
Todos conceitos acima s√£o importantes para entendermos as m√©tricas, mas todas essas m√©tricas s√£o mais facilmente obtidas no `classification_report`.

```
Classification Report:

              precision    recall  f1-score   support

           0       0.69      1.00      0.81        24
           1       1.00      0.08      0.15        12

    accuracy                           0.69        36
   macro avg       0.84      0.54      0.48        36
weighted avg       0.79      0.69      0.59        36
```

### K-Vizinhos mais Pr√≥ximos
Os K-Vizinhos mais Pr√≥ximos, ou KNN (do ingl√™s, K nearest neighbors) √© um dos modelos mais simples de classifica√ß√£o, mas tamb√©m bastante empregado. Seu funcionamento se baseia na classifica√ß√£o de uma inst√¢ncia de acordo com a classe de seus vizinhos mais pr√≥ximos. O n√∫mero k define quantos vizinhos queremos empregar na classifica√ß√£o.

O conceito do Knn √© bastante simples o que permite implementar o algoritmo e verificar o seu funcionamento sem qualquer API ou pacote adicional. Basicamente o modelo consiste na execu√ß√£o de 3 passos:
* Calcular as dist√¢ncias do elemento desejado para os demais
* Encontrar os k-vizinhos mais pr√≥ximos
* Retornar a classe mais frequente entre dos k-vizinhos

#### Normalizando os Dados
O c√°lculo de dist√¢ncias como medida de similaridade (menor dist√¢ncia indicando maior similaridade) pode, entretanto, apresentar grandes desvios quando empregamos vari√°veis ‚Äãcom escalas muito diferentes ou vari√°veis ‚Äã‚Äãnum√©ricas e categ√≥ricas em conjunto. 

V√°rios modelos de aprendizado de m√°quina s√£o baseados em dist√¢ncia como medida de similaridade e s√£o, portanto, sens√≠veis √† normaliza√ß√£o dos dados e devemos aplic√°-la quando empregados dados em diferentes escalas.

Voc√™ pode simplesmente empregar uma fun√ß√£o `minmax_scale` do `scikit-learn` para fazer a normaliza√ß√£o, mas existem outras fun√ß√µes para os demais tipos de normaliza√ß√£o.

```py
from sklearn.preprocessing import minmax_scale

minmax_scale(loans[['age','loan']])
```
```
array([[0.125     , 0.10891089],
       [0.375     , 0.20792079],
       [0.625     , 0.30693069],
       [0.        , 0.00990099],
       [0.375     , 0.5049505 ],
       [0.8       , 0.        ],
       [0.075     , 0.38118812],
       [0.5       , 0.21782178],
       [1.        , 0.40594059],
       [0.7       , 1.        ],
       [0.325     , 0.65346535]])
```

#### Exemplo
```py
import pandas as pd
from sklearn import neighbors
from sklearn.preprocessing import MinMaxScaler

loans = pd.DataFrame({'age':[25,35,45,20,35,52,23,40,60,48,33],
                      'loan':[40000,60000,80000,20000,120000,18000,95000,62000,100000,220000,150000],
                      'default':[1,1,1,1,1,1,0,0,0,0,0] }) # 1='yes'

case  = pd.DataFrame({'age':[47],'loan':[142000]})

X = loans[['age','loan']]      
y = loans.default   

scaler = MinMaxScaler()
scaler.fit(X)
X = scaler.transform(X) 
case_scaled = scaler.transform(case)

clf = neighbors.KNeighborsClassifier(n_neighbors = 3)

clf.fit(X, y)                 

y_pred = clf.predict(case_scaled)

default_pred = ['No','Yes'][y_pred[0]]
print('Default? ', default_pred)
```

### √Årvore de Decis√£o e o M√©todo Partitivo
Uma estrutura de √°rvore 'modela' os dados e divide recursivamente o espa√ßo em regi√µes com classes semelhantes. Essa estrutura faz particionamentos sucessivos dos dados em que as parti√ß√µes s√£o cada vez mais puras, isto √©, s√£o cada vez mais homog√™neas no sentido das classes que cont√™m.

![ArvoreDecisao](https://github.com/Rogerio-mack/BIG_DATA_Analytics_Mineracao_e_Analise_de_Dados/blob/main/figuras/DecisionTreePartitions.png?raw=true)

O n√≥ raiz da √°rvore representa todo o conjunto de dados. Este conjunto √© ent√£o dividido aproximadamente ao meio ao longo de uma dimens√£o (um atributo) por um limite simples. Todos os pontos que possuem um valor  >ùë°  caem no n√≥ filho direito e todos os demais no n√≥ filho esquerdo. O limiar  ùë°  a dimens√£o √© escolhida de forma que os n√≥s filhos resultantes sejam mais puros em termos de associa√ß√£o de classe. O ideal √© que todos os pontos positivos (por exemplo, pontos laranja na figura acima) caiam em um n√≥ filho e todos os pontos negativos (roxos) no outro. Se for esse o caso, a √°rvore est√° pronta. Caso contr√°rio, os n√≥s folha s√£o novamente divididos at√© que eventualmente todas as folhas, ou n√≥s terminais, sejam puras com todos seus pontos com o mesmo r√≥tulo ou n√£o podem ser mais divididos (dois pontos id√™nticos com r√≥tulos diferentes, o que em casos reais podemos de fato ter).

Uma vez que a √°rvore √© constru√≠da os dados de treinamento n√£o precisam ser mais armazenados (como no Knn) pois a √°rvore captura todo o padr√£o dos dados, e voc√™ pode pensar em como uma √∫nica f√≥rmula  ùë¶=ùëé0+ùëé1ùë•  modela um conjunto de dados em uma regress√£o linear. A √°rvore passa a ser nossa 'f√≥rmula' de predi√ß√£o, as entradas de teste simplesmente precisam descer da √°rvore at√© uma folha que contendo a predi√ß√£o da classe, e s√£o muito eficientes. Al√©m disso as √°rvores de decis√£o n√£o requerem m√©tricas porque as divis√µes s√£o baseadas em limites dos valores dos atributos (ou ainda suas propor√ß√µes ou probabilidades) e n√£o em dist√¢ncias.

Agora, um exemplo de como construir uma √°rvore de decis√£o a partir dos dados:

| Name     | sex    | smokes | tie | mask | cape | ears | class |
|----------|--------|--------|-----|------|------|------|-------|
| Batman   | male   | no     | no  | yes  | yes  | yes  | good  |
| Robin    | male   | no     | no  | yes  | yes  | yes  | good  |
| Catwoman | female | no     | no  | yes  | no   | yes  | bad   |
| Joker    | male   | no     | no  | no   | no   | no   | bad   |
| Alfred   | male   | no     | yes | no   | no   | no   | good  |
| Penguin  | male   | yes    | yes | no   | no   | no   | bad   |

| Name     | sex    | smokes | tie | mask | cape | ears | class |
|----------|--------|--------|-----|------|------|------|-------|
| Batgirl  | female | yes    | yes | no   | yes  | no   | ?     |
| Riddler  | male   | yes    | no  | no   | no   | no   | ?     |

O algoritmo de Hunt cria uma √°rvore de decis√£o de maneira recursiva, particionando os registros de treinamento em subconjuntos sucessivamente mais puros. Seja $D_t$ o conjunto de registros de treinamento que atingem um n√≥ $t$. O procedimento recursivo √©, ent√£o, o seguinte:

1. Se $D_t$ cont√©m registros que pertencem √† mesma classe $y_t$, ent√£o $t$ √© um n√≥ folha rotulado como $y_t$

2. Se $D_t$ for um conjunto vazio, ent√£o $t$ √© um n√≥ folha rotulado pela classe padr√£o, $y_d$

3. Se $D_t$ contiver registros que pertencem a mais de uma classe, use um teste de atributo para dividir os dados em subconjuntos menores.

Ele aplica recursivamente o procedimento a cada subconjunto at√© que todos os registros no subconjunto perten√ßam √† mesma classe. O algoritmo de Hunt assume que cada combina√ß√£o de conjuntos de atributos possui um r√≥tulo de classe exclusivo durante o procedimento. Se todos os registros associados a $D_t$ tiverem valores de atributo id√™nticos, exceto para o r√≥tulo da classe, n√£o ser√° poss√≠vel dividir esses registros no futuro. Nesse caso, o n√≥ √© classificado como um n√≥ folha com o mesmo r√≥tulo de classe que a classe principal de registros de treinamento associados a este n√≥.

![ArvoreFuncionando](https://github.com/Rogerio-mack/BIG_DATA_Analytics_Mineracao_e_Analise_de_Dados/blob/main/figuras/DecisionTree3.png?raw=true)

> *Note: √Årvores de Decis√£o n√£o requerem valores num√©ricos e podem empregar valores categ√≥ricos diretamente. Entretanto, o estimador do scikit-learn n√£o est√° implementado deste modo e requer que os valores dos atributos sejam num√©ricos. Por isso, √© necess√°rio fazer o encode dos dados antes de aplicar o estimador.*

#### Entropia e Ganho de Informa√ß√£o
A escolha de atributos para a defini√ß√£o dos n√≥s da √°rvore segue o crit√©rio de empregar os atributos com maior ganho de informa√ß√£o para maximizar a pureza dos particionamentos. Voc√™ pode entender o ganho de informa√ß√£o como uma medida de quanto a informa√ß√£o de um atributo contribui para diminuir a incerteza sobre outro.

A *Entropia* de um atributo √© o n√≠vel m√©dio de informa√ß√£o, surpresa, ou ainda incerteza, inerente aos resultados poss√≠veis desse atributo.

O *Ganho de Informa√ß√£o* de um atributo com rela√ß√£o a uma vari√°vel classe objetivo ( ùëá , Target) √© uma medida de quanto a informa√ß√£o desse atributo diminui a incerteza sobre a classe dos dados, ou seu valor no caso de uma regress√£o.

## Aprendizado N√£o Supervisionado
S√£o muitas as situa√ß√µes em que n√£o temos a possibilidade de termos dados de treinamento pr√©-rotulados. Apesar disso, queremos extrair conhecimentos √∫teis dos dados para a tomada de decis√µes e a√ß√µes. √â nesta situa√ß√£o que modelos de aprendizado n√£o supervisionado s√£o bastante √∫teis. N√£o havendo um conjunto de entradas e sa√≠das, isto √© um conjunto de treinamento, o objetivo do aprendizado n√£o supervisionado, ser√° o de descobrir padr√µes nos dados. O modelo ou algoritmo ir√° tentar aprender estruturas, rela√ß√µes e padr√µes latentes nos dados sem qualquer assist√™ncia ou supervis√£o.

Algoritmos de aprendizado n√£o supervisionado normalmente incluem: 

* Algoritmos de clusteriza√ß√£o (ou agrupamento) 
* Detec√ß√£o de anomalias
* Redu√ß√£o de Dimensionalidade (ou M√©todos de Vari√°veis Latentes) 
* Regras de Associa√ß√£o
       
Os m√©todos de clusteriza√ß√£o incluem clusteriza√ß√£o hier√°rquica, k-m√©dias. Para detec√ß√£o de anomalias h√° m√©todos como Fator Outlier Local e Floresta de Isolamento. A an√°lise de componentes principais (PCA) e valor singular decomposi√ß√£o (SVD) s√£o m√©todos empregados para redu√ß√£o de dimensionalidade e algoritmos *apriori* s√£o aplicados para busca de regras de associa√ß√£o.

Em todos esses casos h√° certamente padr√µes particulares nos dados que aparecem com mais frequ√™ncia e outros menos, e queremos descobrir o que acontece e o que n√£o acontece nos dados. Isso, em estat√≠stica, denominado de estimativa de densidade.

### Aprendizado Supervisionado vs N√£o Supervisionado
